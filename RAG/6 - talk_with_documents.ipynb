{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "from langchain_community.document_loaders.pdf import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 16 0 (offset 0)\n",
      "Ignoring wrong pointing object 18 0 (offset 0)\n",
      "Ignoring wrong pointing object 20 0 (offset 0)\n",
      "Ignoring wrong pointing object 22 0 (offset 0)\n",
      "Ignoring wrong pointing object 42 0 (offset 0)\n",
      "Ignoring wrong pointing object 50 0 (offset 0)\n",
      "Ignoring wrong pointing object 52 0 (offset 0)\n",
      "Ignoring wrong pointing object 54 0 (offset 0)\n",
      "Ignoring wrong pointing object 56 0 (offset 0)\n",
      "Ignoring wrong pointing object 58 0 (offset 0)\n",
      "Ignoring wrong pointing object 70 0 (offset 0)\n",
      "Ignoring wrong pointing object 72 0 (offset 0)\n",
      "Ignoring wrong pointing object 89 0 (offset 0)\n",
      "Ignoring wrong pointing object 91 0 (offset 0)\n",
      "Ignoring wrong pointing object 103 0 (offset 0)\n",
      "Ignoring wrong pointing object 108 0 (offset 0)\n",
      "Ignoring wrong pointing object 149 0 (offset 0)\n",
      "Ignoring wrong pointing object 155 0 (offset 0)\n",
      "Ignoring wrong pointing object 158 0 (offset 0)\n",
      "Ignoring wrong pointing object 160 0 (offset 0)\n",
      "Ignoring wrong pointing object 163 0 (offset 0)\n",
      "Ignoring wrong pointing object 165 0 (offset 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146\n"
     ]
    }
   ],
   "source": [
    "paths = [\n",
    "    \"files/apostila.pdf\",\n",
    "    \"files/LLM.pdf\",\n",
    "]\n",
    "\n",
    "pages = []\n",
    "\n",
    "## Cria uma lista com todos os documentos\n",
    "for path in paths:\n",
    "    loader = PyPDFLoader(path)\n",
    "    docs = loader.load()\n",
    "    pages.extend(docs)\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500, chunk_overlap=50, separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    ")\n",
    "\n",
    "docs = splitter.split_documents(pages)\n",
    "\n",
    "normalized_docs = []\n",
    "\n",
    "for doc in docs:\n",
    "    # Remove repeated dots, dashes and spaces\n",
    "    cleaned_text = doc.page_content\n",
    "    cleaned_text = \" \".join(cleaned_text.split())  # Remove extra whitespace\n",
    "    cleaned_text = cleaned_text.replace(\"....\", \"\")\n",
    "    cleaned_text = cleaned_text.replace(\"...\", \"\")\n",
    "    cleaned_text = cleaned_text.replace(\"..\", \"\")\n",
    "    cleaned_text = cleaned_text.replace(\"----\", \"-\")\n",
    "    cleaned_text = cleaned_text.replace(\"---\", \"-\")\n",
    "    cleaned_text = cleaned_text.replace(\"--\", \"-\")\n",
    "    cleaned_text = cleaned_text.replace(\"  \", \"\")\n",
    "    cleaned_text = cleaned_text.strip()\n",
    "\n",
    "    # Create new document with cleaned text\n",
    "    doc.page_content = cleaned_text\n",
    "    normalized_docs.append(doc)\n",
    "\n",
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "## Banco vetorial\n",
    "\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "dir = \"db/chroma_db_vectorstore\"\n",
    "\n",
    "vector_store = Chroma.from_documents(\n",
    "    documents=docs,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.retrieval_qa.base import RetrievalQA\n",
    "\n",
    "chat_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm, chain_type=\"stuff\", retriever=vector_store.as_retriever(search_type=\"mmr\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hugging Face é uma comunidade de código aberto que reúne uma ampla variedade de modelos de Inteligência Artificial contribuídos por centenas de milhares de desenvolvedores. Esses modelos podem ser utilizados para uma variedade de casos de uso, como geração de texto, resumo e classificação. Embora a comunidade esteja rapidamente alcançando o desempenho de modelos proprietários, como o GPT-4, ainda há desafios a serem superados para igualar o desempenho dos modelos mais avançados. No entanto, a colaboração e a diversidade de modelos disponíveis na Hugging Face tornam-na uma ferramenta poderosa para desenvolvedores e pesquisadores que buscam soluções de Inteligência Artificial inovadoras e eficazes.\n",
      "\n",
      "Além disso, é importante ressaltar que o aprendizado em Python é fundamental para quem deseja se aprofundar em Inteligência Artificial. O Python oferece uma ampla gama de recursos, como orientação a objetos, programação funcional, metaprogramação, interface gráfica, expressões regulares, threads, tratamento de exceções, funções anônimas, geradores, desenvolvimento web e aplicativos móveis. Portanto, ao ingressar no mundo Python, os desenvolvedores têm a oportunidade de explorar diversas áreas e expandir seus conhecimentos para se tornarem profissionais mais completos e versáteis.\n",
      "\n",
      "Além disso, a Databricks é uma empresa líder no setor de dados e IA, com mais de 9.000 organizações em todo o mundo confiando em sua Plataforma Databricks Lakehouse para unificar dados, análises e IA. Empresas renomadas, como Comcast, Condé Nast e mais de 50% da Fortune 500, utilizam os serviços da Databricks para impulsionar suas operações e inovações. Com sede em São Francisco e escritórios em todo o mundo, a Databricks desempenha um papel fundamental no avanço da tecnologia de dados e IA em escala global. A integração de ferramentas como a Databricks com a comunidade da Hugging Face pode resultar em soluções ainda mais poderosas e abrangentes para os desafios enfrentados por desenvolvedores e pesquisadores na área de Inteligência Artificial.\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# Template melhorado\n",
    "question_prompt_template = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "Você é um especialista em Inteligência Artificial e Modelos de Linguagem.\n",
    "\n",
    "Baseado apenas nas informações abaixo dos documentos, responda à pergunta de forma completa, precisa e didática. Não invente informações — responda apenas com base no que foi encontrado.\n",
    "\n",
    "Documentos:\n",
    "{context_str}\n",
    "\n",
    "Pergunta:\n",
    "{question}\n",
    "\n",
    "Resposta:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "refine_prompt_template = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "Você já escreveu uma resposta inicial baseada em alguns documentos.\n",
    "\n",
    "Aqui está a resposta inicial:\n",
    "{existing_answer}\n",
    "\n",
    "Baseado no novo documento abaixo, você pode melhorar e expandir a resposta anterior, se necessário. Seja preciso e mantenha a didática.\n",
    "\n",
    "Novo Documento:\n",
    "{context_str}\n",
    "\n",
    "Se o novo documento não trouxer informações relevantes, mantenha a resposta original.\n",
    "\n",
    "Resposta Refinada:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "# Agora passa esse template pra RetrievalQA\n",
    "chat_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=vector_store.as_retriever(search_type=\"mmr\"),\n",
    "    chain_type=\"refine\",\n",
    "    chain_type_kwargs={\n",
    "        \"question_prompt\": question_prompt_template,\n",
    "        \"refine_prompt\": refine_prompt_template,\n",
    "    },\n",
    ")\n",
    "\n",
    "\n",
    "ask = \"O que é Hugging Face?\"\n",
    "\n",
    "response = chat_chain.invoke({\"query\": ask})\n",
    "\n",
    "print(response[\"result\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
